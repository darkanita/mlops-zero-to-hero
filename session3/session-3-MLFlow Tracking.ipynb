{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Seguimiento de Experimentos con MLflow\n",
    "\n",
    "El ciclo de vida del machine learning implica entrenar múltiples algoritmos, usar diferentes hiperparámetros y librerías, y obtener distintos resultados y modelos entrenados. Esta lección explora cómo realizar un seguimiento de esos experimentos para organizar el ciclo de vida del machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En esta leción, tú:\n",
    "* Introducirás el seguimiento de experimentos de ML con MLflow.\n",
    "* Registrarás un experimento y explorarás los resultados en la Interfaz de Usuario (UI).\n",
    "* Guardarás parámetros, métricas y un modelo.\n",
    "* Consultarás ejecuciones pasadas de forma programática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El Desafío de la Organización\n",
    "\n",
    "A lo largo del ciclo de vida del machine learning:\n",
    "* Los científicos de datos prueban muchos modelos diferentes.\n",
    "* Usan diversas librerías.\n",
    "* Cada una con diferentes hiperparámetros.\n",
    "\n",
    "Hacer un seguimiento de estos resultados plantea un desafío de organización, que incluye el almacenamiento de:\n",
    "* Experimentos\n",
    "* Resultados\n",
    "* Modelos\n",
    "* Artefactos suplementarios\n",
    "* Código\n",
    "* Versiones de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del Entorno\n",
    "\n",
    "Antes de empezar, asegúrate de tener las librerías necesarias. Puedes instalarlas usando `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomenta y ejecuta la siguiente línea si no tienes las librerías instaladas\n",
    "# !pip install mlflow scikit-learn pandas matplotlib hyperopt shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver tus experimentos en la interfaz de usuario de MLflow, abre una terminal, navega a la carpeta donde estás ejecutando este notebook y ejecuta el siguiente comando. Esto iniciará un servidor local y creará una carpeta `mlruns` para almacenar todos los datos de tus experimentos.\n",
    "\n",
    "`mlflow ui`\n",
    "\n",
    "Por defecto, la interfaz estará disponible en `http://127.0.0.1:5000`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seguimiento de Experimentos con MLflow\n",
    "\n",
    "MLflow Tracking es:\n",
    "* Una API de registro específica para machine learning.\n",
    "* Independiente de las librerías y entornos que realizan el entrenamiento.\n",
    "* Organizada en torno al concepto de **runs (ejecuciones)**, que son ejecuciones de código de ciencia de datos.\n",
    "* Las ejecuciones se agrupan en **experimentos**.\n",
    "* Un servidor de MLflow puede alojar muchos experimentos.\n",
    "\n",
    "Cada ejecución puede registrar la siguiente información:\n",
    "* **Parameters (Parámetros):** Pares clave-valor de parámetros de entrada, como el número de árboles en un modelo de Random Forest.\n",
    "* **Metrics (Métricas):** Métricas de evaluación como RMSE o el Área Bajo la Curva ROC.\n",
    "* **Artifacts (Artefactos):** Archivos de salida arbitrarios en cualquier formato. Esto puede incluir imágenes, modelos serializados (pickled) y archivos de datos.\n",
    "* **Source (Fuente):** El código que originó el experimento.\n",
    "\n",
    "<div><img src=\"https://mlflow.org/docs/latest/assets/images/tracking-basics-dd24b77b7d7b32c5829e257316701801.png\" style=\"height: 400px; margin: 20px\"/></div>\n",
    "\n",
    "### Carga de Datos y Librerías\n",
    "\n",
    "Primero, importemos las librerías necesarias y preparemos nuestro conjunto de datos de \"California Housing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing[\"data\"]\n",
    "y = housing[\"target\"]\n",
    "\n",
    "# Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tu Primer Experimento\n",
    "\n",
    "Comencemos un experimento usando `mlflow.start_run()`. Dentro del bloque `with`, podemos registrar un modelo, métricas y más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar experimento\n",
    "mlflow.set_experiment(\"Basic RF Run\")\n",
    "\n",
    "# Inicia un experimento con mlflow.start_run()\n",
    "with mlflow.start_run() as run:\n",
    "    # Crea el modelo, entrénalo y haz predicciones\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    predictions = rf.predict(X_test)\n",
    "\n",
    "    # Registra el modelo usando mlflow.sklearn.log_model()\n",
    "    mlflow.sklearn.log_model(rf, \"random_forest_model\")\n",
    "\n",
    "    # Calcula y registra las métricas del modelo\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    metrics = {\"mse\": mse, \"rmse\": rmse, \"r2\": r2}\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    # Obtén el ID del experimento y de la ejecución\n",
    "    run_id = run.info.run_id\n",
    "    experiment_id = run.info.experiment_id\n",
    "    \n",
    "    print(f\"Run ID: {run_id}\")\n",
    "    print(f\"Experiment ID: {experiment_id}\")\n",
    "    print(f\"Dentro de una ejecución de MLflow con run_id `{run_id}` y experiment_id `{experiment_id}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Ahora ve a la interfaz de MLflow (`http://127.0.0.1:5000`) en tu navegador para ver el experimento y la ejecución que acabas de registrar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros, Métricas y Artefactos\n",
    "\n",
    "¡Pero espera, hay más! En el último ejemplo, registraste el nombre de la ejecución, métricas de evaluación y tu modelo como un artefacto. Ahora vamos a registrar **parámetros**, múltiples métricas y otros artefactos, como la importancia de las características (feature importances).\n",
    "\n",
    "Para registrar artefactos como archivos, primero debemos guardarlos en disco para que MLflow pueda luego subirlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def log_rf(experiment_id, run_name, params, X_train, X_test, y_train, y_test):\n",
    "  \n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name=run_name) as run:\n",
    "        # Crea, entrena y predice con el modelo\n",
    "        rf = RandomForestRegressor(**params)\n",
    "        rf.fit(X_train, y_train)\n",
    "        predictions = rf.predict(X_test)\n",
    "\n",
    "        # Registra el modelo\n",
    "        mlflow.sklearn.log_model(rf, \"random_forest_model\")\n",
    "\n",
    "        # Registra los parámetros\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Registra las métricas\n",
    "        mlflow.log_metrics({\n",
    "            \"rmse\": np.sqrt(mean_squared_error(y_test, predictions)),\n",
    "            \"mse\": mean_squared_error(y_test, predictions), \n",
    "            \"mae\": mean_absolute_error(y_test, predictions), \n",
    "            \"r2\": r2_score(y_test, predictions)\n",
    "        })\n",
    "\n",
    "        # Registra la importancia de las características\n",
    "        importance = (pd.DataFrame(list(zip(housing.feature_names, rf.feature_importances_)), columns=[\"Feature\", \"Importance\"])\n",
    "                      .sort_values(\"Importance\", ascending=False))\n",
    "        \n",
    "        # Guarda el artefacto en un archivo local temporalmente\n",
    "        importance_path = \"importance.csv\"\n",
    "        importance.to_csv(importance_path, index=False)\n",
    "        mlflow.log_artifact(importance_path, \"feature-importance\")\n",
    "\n",
    "        # Registra un gráfico de la importancia de las características\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        importance.plot.bar(ax=ax)\n",
    "        plt.title(\"Feature Importances\")\n",
    "        mlflow.log_figure(fig, \"feature_importances.png\")\n",
    "\n",
    "        return run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar con Diferentes Parámetros\n",
    "\n",
    "Ahora, usemos nuestra función para ejecutar y registrar dos modelos con diferentes hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecución 1\n",
    "params_1 = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 5,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "log_rf(experiment_id, \"Second Run\", params_1, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Ejecución 2\n",
    "params_2 = {\n",
    "    \"n_estimators\": 1000,\n",
    "    \"max_depth\": 10,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "log_rf(experiment_id, \"Third Run\", params_2, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultando Ejecuciones Anteriores\n",
    "\n",
    "Puedes consultar ejecuciones pasadas de forma programática para usar sus datos de vuelta en Python. La forma de hacerlo es a través de un objeto **`MlflowClient`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar y Filtrar Ejecuciones\n",
    "\n",
    "Usemos `.search_runs()` para listar todas las ejecuciones de nuestro experimento o para filtrarlas según un criterio. La función devuelve un DataFrame de Pandas con los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar todas las ejecuciones en nuestro experimento\n",
    "runs_df = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "print(\"Todas las ejecuciones:\")\n",
    "display(runs_df[[\"run_id\", \"metrics.rmse\", \"params.n_estimators\", \"params.max_depth\"]])\n",
    "\n",
    "# Buscar ejecuciones que cumplan una condición\n",
    "filtered_runs_df = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"metrics.r2 > 0.6\"\n",
    ")\n",
    "print(\"\\nEjecuciones con R2 > 0.6:\")\n",
    "display(filtered_runs_df[[\"run_id\", \"metrics.r2\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acceder a los Artefactos de una Ejecución\n",
    "\n",
    "Podemos obtener la mejor ejecución (por ejemplo, la que tiene el menor RMSE) y ver sus artefactos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar la mejor ejecución ordenando por RMSE\n",
    "best_run = filtered_runs_df.sort_values(\"metrics.rmse\").iloc[0]\n",
    "best_run_id = best_run.run_id\n",
    "print(f\"Mejor Run ID: {best_run_id}\")\n",
    "\n",
    "# Listar los artefactos de la mejor ejecución\n",
    "artifacts = client.list_artifacts(best_run_id)\n",
    "for artifact in artifacts:\n",
    "    print(f\"- {artifact.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recargar un Modelo Registrado\n",
    "\n",
    "Finalmente, podemos recargar el modelo de nuestra mejor ejecución directamente desde MLflow para usarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recargar el modelo\n",
    "model_uri = f\"runs:/{best_run_id}/random_forest_model\"\n",
    "reloaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "# Usar el modelo recargado\n",
    "print(\"\\nImportancia de las características del modelo recargado:\")\n",
    "print(reloaded_model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firmas y Ejemplos de Entrada\n",
    "\n",
    "Es una buena práctica registrar un modelo con una **firma (signature)** y un **ejemplo de entrada (input example)**. Esto permite mejores comprobaciones de esquema y facilita la integración con herramientas de despliegue automático.\n",
    "\n",
    "* **Firma:** Es el esquema de las entradas y salidas del modelo.\n",
    "* **Ejemplo de Entrada:** Son algunas filas de datos de ejemplo que el modelo espera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "with mlflow.start_run(run_name=\"Signature Example\") as run:\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf_model = rf.fit(X_train, y_train)\n",
    "    mse = mean_squared_error(rf_model.predict(X_test), y_test)\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "\n",
    "    # Infiere la firma y crea un ejemplo de entrada\n",
    "    signature = infer_signature(X_train, pd.DataFrame(y_train))\n",
    "    input_example = X_train[0:3]\n",
    "    \n",
    "    # Registra el modelo con la firma y el ejemplo de entrada\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=rf_model, \n",
    "        artifact_path=\"rf_model_with_signature\", \n",
    "        signature=signature, \n",
    "        input_example=input_example\n",
    "    )\n",
    "    \n",
    "    print(f\"Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecuciones Anidadas (Nested Runs)\n",
    "\n",
    "Las ejecuciones anidadas son una herramienta organizativa útil que permite crear una estructura de árbol con ejecuciones \"padre\" e \"hijo\". Son ideales para agrupar las diferentes pruebas de un proceso de **ajuste de hiperparámetros**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Nested Example Parent\") as parent_run:\n",
    "    print(f\"Parent Run ID: {parent_run.info.run_id}\")\n",
    "    \n",
    "    # Crea una ejecución anidada con el argumento nested=True\n",
    "    with mlflow.start_run(run_name=\"Child 1\", nested=True):\n",
    "        mlflow.log_param(\"run_name\", \"child_1\")\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Child 2\", nested=True):\n",
    "        mlflow.log_param(\"run_name\", \"child_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autologging\n",
    "\n",
    "Hasta ahora, hemos registrado todo manualmente. **Autologging** permite registrar métricas, parámetros y modelos sin necesidad de llamadas explícitas a `log_`.\n",
    "\n",
    "Simplemente llama a `mlflow.autolog()` antes de tu código de entrenamiento.\n",
    "\n",
    "**NOTA:** Con autologging, no es necesario usar un bloque `with mlflow.start_run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.autolog()\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42, n_estimators=200, max_depth=8)\n",
    "rf_model = rf.fit(X_train, y_train)\n",
    "\n",
    "# Desactiva autologging para las siguientes celdas\n",
    "mlflow.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ve a la UI de MLflow. Verás que se ha creado una nueva ejecución con todos los parámetros, métricas y el modelo registrados automáticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste de Hiperparámetros con Hyperopt\n",
    "\n",
    "Uno de los casos de uso más comunes para las ejecuciones anidadas y el autologging es el ajuste de hiperparámetros. Usaremos la librería **Hyperopt** para encontrar el mejor modelo de Random Forest.\n",
    "\n",
    "**Nota sobre `Trials` vs `SparkTrials`:** El notebook original usaba `SparkTrials` para afinación paralela en Databricks. En un entorno local, usaremos `Trials`, que ejecuta los trabajos de forma secuencial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "\n",
    "# 1. Define la función objetivo que Hyperopt minimizará (en este caso, el MSE)\n",
    "def objective(params):\n",
    "    # Habilita el autologging para cada prueba de hyperopt\n",
    "    mlflow.autolog(log_models=False, silent=True) # No guardamos el modelo en cada iteración para ahorrar espacio\n",
    "    \n",
    "    with mlflow.start_run(nested=True):\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=int(params[\"n_estimators\"]), \n",
    "            max_depth=int(params[\"max_depth\"]), \n",
    "            min_samples_leaf=int(params[\"min_samples_leaf\"]),\n",
    "            min_samples_split=int(params[\"min_samples_split\"]),\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        score = mean_squared_error(pred, y_test)\n",
    "\n",
    "    # Hyperopt minimiza el valor de retorno ('loss')\n",
    "    return score\n",
    "\n",
    "# 2. Define el espacio de búsqueda para los hiperparámetros\n",
    "search_space = {\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 100, 500, 5),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 5, 20, 1),\n",
    "    \"min_samples_leaf\": hp.quniform(\"min_samples_leaf\", 1, 5, 1),\n",
    "    \"min_samples_split\": hp.quniform(\"min_samples_split\", 2, 6, 1)\n",
    "}\n",
    "\n",
    "# 3. Ejecuta Hyperopt\n",
    "with mlflow.start_run(run_name=\"Hyperopt Tuning\"):\n",
    "    best_params = fmin(\n",
    "        fn=objective,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=16,\n",
    "        trials=Trials()\n",
    "    )\n",
    "\n",
    "print(\"Mejores parámetros encontrados:\")\n",
    "print(best_params)\n",
    "\n",
    "# Desactiva autologging\n",
    "mlflow.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la UI de MLflow, verás una ejecución padre \"Hyperopt Tuning\" con 16 ejecuciones hijas anidadas, una por cada combinación de hiperparámetros probada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos Adicionales\n",
    "\n",
    "* [Documentación de Hyperopt](http://hyperopt.github.io/hyperopt/)\n",
    "* [Documentación de MLflow SHAP](https://www.mlflow.org/docs/latest/python_api/mlflow.shap.html)\n",
    "* [Blog sobre ajuste de hiperparámetros con MLflow y Hyperopt](https://databricks.com/blog/2019/06/07/hyperparameter-tuning-with-mlflow-apache-spark-mllib-and-hyperopt.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
